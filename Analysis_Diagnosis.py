import os
import json
from datetime import datetime
from opensearchpy import OpenSearch, RequestsHttpConnection
import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_openai import ChatOpenAI
from langchain.agents import AgentType, initialize_agent
from langchain_community.callbacks import get_openai_callback
import re
from dotenv import load_dotenv

# Load envs
# Load envs

load_dotenv()

DEFAULT_PROMPT_TEMPLATE = '''You are a Senior Pega Lead System Architect (LSA) and low-level Pega engine expert specializing in clipboard internals, activity execution, data transforms, and JSON/page conversion functions.

I will provide error-group data, rule definitions, activities, data transforms, Java steps, and logs from a Pega system.

DATA PROVIDED:
{context_str}

Your task is NOT to produce a generic incident report.

Your task is to perform deep technical forensic analysis and identify the exact failure point at rule/activity/property/function level.

You must determine precisely:

WHERE THE ERROR OCCURS

Identify the exact activity name

Identify the exact step (Step number, method name such as Property-Set, Page-Copy, Apply-DataTransform, Java step, etc.)

Identify the exact function or API causing the failure (example: pxConvertPageToStringWithZone, pxConvertStringToPage, Property-Set, Page-Copy, etc.)

Identify the exact clipboard page and property involved

Identify the exact rule, data transform, or Java logic involved

WHY THE ERROR OCCURS

Explain the precise technical cause at clipboard/property structure level

Identify invalid page structure, missing property definition, null PageList entries, class mismatch, or invalid JSON conversion if applicable

Explain what condition causes the failure inside Pega engine internals

Reference specific properties, PageLists, or conversion functions causing failure

TRACE THE COMPLETE FAILURE FLOW
Show step-by-step execution flow like:

Activity → Step → Function → Clipboard Page → Property → Failure

IDENTIFY THE EXACT FAULTY CODE OR STEP
Quote the exact failing line or step from activity, Java code, or data transform.

PROVIDE EXACT RESOLUTION STEPS
Provide specific, implementable fixes such as:

exact activity step to modify

exact function replacement

exact code fix

exact guard condition to add

exact data transform fix

exact property/class correction

DO NOT GIVE GENERIC STATEMENTS
DO NOT say:

"data mapping issue"

"configuration issue"

"needs investigation"

Instead say exactly:

which property

which page

which function

which activity step

which rule

OUTPUT FORMAT (STRICT)

Return answer in this exact structure:

ERROR LOCATION
(detailed precise location)

ROOT CAUSE
(technical clipboard-level explanation)

FAILURE FLOW
(step-by-step execution path)

FAULTY STEP OR CODE
(exact activity step, Java code, or function)

RESOLUTION
(exact fix with step-by-step instructions)

PREVENTION
(optional guardrails to prevent recurrence)
"Assume you have full knowledge of Pega clipboard internals, activity execution engine, and JSON conversion functions. Your answer must identify the exact failing function and property."'''

def clean_markdown(text):
    """
    Strips markdown formatting to return clean plain text.
    Removes: **bold**, ## Headers, `code`, and ```blocks```.
    """
    # Remove code block markers
    text = re.sub(r'```[a-zA-Z]*\n?', '', text)
    text = re.sub(r'```', '', text)
    
    # Remove inline code backticks
    text = re.sub(r'`', '', text)
    
    # Remove bold/italic markers (* or _)
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)  # **bold**
    text = re.sub(r'\*([^*]+)\*', r'\1', text)      # *italic*
    text = re.sub(r'__([^_]+)__', r'\1', text)      # __bold__
    text = re.sub(r'_([^_]+)_', r'\1', text)        # _italic_
    
    # Remove headers (### Header)
    text = re.sub(r'^#+\s*', '', text, flags=re.MULTILINE)
    
    return text.strip()

def get_opensearch_client():
    OPENSEARCH_URL = os.environ.get("OPENSEARCH_URL")
    OPENSEARCH_USER = os.environ.get("OPENSEARCH_USER")
    OPENSEARCH_PASS = os.environ.get("OPENSEARCH_PASS")
    CLIENT_TIMEOUT = int(os.environ.get("CLIENT_TIMEOUT", 60))

    if not OPENSEARCH_URL or not OPENSEARCH_USER or not OPENSEARCH_PASS:
        raise ValueError("Missing required OpenSearch environment variables: OPENSEARCH_URL, OPENSEARCH_USER, OPENSEARCH_PASS")

    client = OpenSearch(
        hosts=[OPENSEARCH_URL],
        http_auth=(OPENSEARCH_USER, OPENSEARCH_PASS),
        verify_certs=False,
        ssl_show_warn=False,
        timeout=CLIENT_TIMEOUT,
        max_retries=5,
        retry_on_timeout=True,
        retry_on_status=(500, 502, 503, 504)
    )
    return client

def fetch_grouped_errors(client, size=5):
    """
    Fetch top grouped errors from pega-analysis-results.
    Prioritizes groups with 'PENDING' diagnosis or just largest counts.
    """
    index = "pega-analysis-results"
    
    # Check if index exists first
    if not client.indices.exists(index=index):
        print(f"Index {index} does not exist. Run log_grouper.py first.")
        return []

    query = {
        "size": size,
        "query": {
            "bool": {
                "must": [
                    {"term": {"diagnosis.status": "PENDING"}}
                ]
            }
        },
        "sort": [
            {"count": {"order": "desc"}}
        ]
    }
    
    response = client.search(body=query, index=index)
    hits = response['hits']['hits']
    
    results = []
    for hit in hits:
        doc = hit['_source']
        doc['_id'] = hit['_id']
        results.append(doc)
        
    return results



def update_diagnosis_in_opensearch(client, doc_id, diagnosis_text, token_usage=None):
    """
    Update the grouped document with diagnosis results.
    """
    index = "pega-analysis-results"
    
    body = {
        "doc": {
            "diagnosis": {
                "status": "DIAGNOSIS COMPLETED",
                "report": diagnosis_text,
                "report": diagnosis_text,
                "timestamp": datetime.utcnow().isoformat(),
                "token_usage": token_usage or {}
            }
        }
    }
    
    try:
        client.update(index=index, id=doc_id, body=body)
        print(f"Updated diagnosis for group {doc_id}")
    except Exception as e:
        print(f"Failed to update diagnosis for {doc_id}: {e}")



def construct_analysis_context(group_doc, pega_api_response=None):
    """
    Helper to construct the analysis context dictionary from a group document.
    Returns the full group document to ensure all fields (including rules) are available to the LLM.
    Optionally includes Pega API response if available.
    """
    context = group_doc.copy()
    
    if pega_api_response:
        context['pega_api_insights'] = pega_api_response
    
    return context

async def execute_diagnosis(agent, prompt):
    """
    Executes the diagnosis using the provided agent and prompt.
    Returns: (diagnosis_text, token_usage)
    """
    try:
        with get_openai_callback() as cb:
            response = await agent.ainvoke({"input": prompt})
            raw_text = response["output"]
            diagnosis_text = clean_markdown(raw_text)
            
            token_usage = {
                "total_tokens": cb.total_tokens,
                "prompt_tokens": cb.prompt_tokens,
                "completion_tokens": cb.completion_tokens,
                "total_cost": cb.total_cost
            }
            print(f"  Diagnosis Cost: ${cb.total_cost:.4f} (Tokens: {cb.total_tokens})")
            return diagnosis_text, token_usage
    except Exception as exc:
        print(f"Failed to execute diagnosis: {exc}")
        return None, None

async def diagnose_single_group(client, group_id, prompt_template=None, pega_api_response=None):
    """
    Standalone function to diagnose a single group by ID.
    Used by the Dashboard for on-demand analysis.
    """
    try:
        # 1. Fetch Group Data
        group_doc = client.get(index="pega-analysis-results", id=group_id)
        if not group_doc or '_source' not in group_doc:
             return "Group not found or deleted.", {}
        
        source = group_doc['_source']
        analysis_context = construct_analysis_context(source, pega_api_response)
        context_str = json.dumps(analysis_context, indent=2)

        # 2. Setup Agent (Re-using logic from main flow, could be optimized to pass agent in)
        mcp_server_config = {
            "opensearch": { 
                "url": os.getenv("MCP_SERVER_URL", "http://localhost:9900"),
                "transport": "sse",
                "headers": {"Content-Type": "application/json", "Accept-Encoding": "identity"}
            }
        }
        
        mcp_client = MultiServerMCPClient(mcp_server_config)
        tools = await mcp_client.get_tools()
        llm = ChatOpenAI(model="gpt-4o")
        agent = initialize_agent(
            tools=tools,
            llm=llm,
            agent=AgentType.OPENAI_FUNCTIONS,
            verbose=True,
            handle_parsing_errors=True
        )

        # 3. Construct Prompt
        if not prompt_template:
            # Default Prompt
            prompt = DEFAULT_PROMPT_TEMPLATE.format(context_str=context_str)
        else:
            # Inject context into user provided template if placeholder exists, else append
            if "{context_str}" in prompt_template:
                prompt = prompt_template.format(context_str=context_str)
            else:
                prompt = f"{prompt_template}\n\nData Provided:\n{context_str}"

        # 4. Execute
        diagnosis_text, token_usage = await execute_diagnosis(agent, prompt)
        
        # 5. Update OpenSearch
        if diagnosis_text:
             update_diagnosis_in_opensearch(client, group_id, diagnosis_text, token_usage)
             
        return diagnosis_text, token_usage

    except Exception as e:
        print(f"Error in diagnose_single_group: {e}")
        return f"Error: {str(e)}", {}



async def run_diagnosis_workflow():
    print("Starting Log Diagnosis Workflow (LangChain)")
    client = get_opensearch_client()
    
    # 1. Fetch Grouped Errors
    grouped_errors = fetch_grouped_errors(client)
    print(f"Found {len(grouped_errors)} pending error groups to diagnose")
    
    if not grouped_errors:
        print("No pending error groups found. Make sure log_grouper.py has run.")
        return

    # 2. Define MCP Server URL
    mcp_server_config = {
        "opensearch": { 
            "url": os.getenv("MCP_SERVER_URL", "http://localhost:9900"),
            "transport": "sse",
            "headers": {
                "Content-Type": "application/json",
                "Accept-Encoding": "identity",
            }
        }
    }
    
    try:
        mcp_client = MultiServerMCPClient(mcp_server_config)
        tools = await mcp_client.get_tools() 
        print(f"Fetched {len(tools)} tools from MCP server")


        MODEL_NAME = "gpt-4o"
        llm = ChatOpenAI(model=MODEL_NAME)

        # 3. Initialize LangChain Agent
        agent = initialize_agent(
            tools=tools,
            llm=llm,
            agent=AgentType.OPENAI_FUNCTIONS,
            verbose=True,
            handle_parsing_errors=True
        )

        # 4. Process each group
        for group_doc in grouped_errors:
            group_id = group_doc['_id']
            
            # Construct Context
            analysis_context = construct_analysis_context(group_doc)
            
            context_str = json.dumps(analysis_context, indent=2)
            
            print(f"Diagnosing Group: {group_doc.get('group_signature')} (Count: {group_doc.get('count')})")

            # Define Prompt
            prompt = DEFAULT_PROMPT_TEMPLATE.format(context_str=context_str)

            # Invoke Agent
            diagnosis_text, token_usage = await execute_diagnosis(agent, prompt)
            
            if diagnosis_text:
                update_diagnosis_in_opensearch(client, group_id, diagnosis_text, token_usage)
            
    except Exception as e:
        print(f"Error in diagnosis workflow: {e}")
        import traceback
        traceback.print_exc()
        # Handle TaskGroup exceptions explicitly if present
        if hasattr(e, 'exceptions'):
            for sub_exc in e.exceptions:
                print(f"Sub-exception: {sub_exc}")

if __name__ == "__main__":
    asyncio.run(run_diagnosis_workflow())